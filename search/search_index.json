{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>Series of labs and instructions to introduce you to containers and Docker. Learn to run a container, inspect a container and understand the isolation of processes, create a Dockerfile, build an image from a Dockerfile and understand layers, tag and push images to a registry, scale and update containers, and more.</p>"},{"location":"#about-this-workshop","title":"About this workshop","text":"<p>This series has an additional presentation.</p>"},{"location":"#agenda","title":"Agenda","text":"Lab 0 Pre-work Lab 1 Run your first container Lab 2 Add value with custom images Lab 3 Manage data in containers"},{"location":"#pre-requirements","title":"Pre-requirements","text":"<p>For this workshop you must have:</p> <ul> <li>Docker CLI,</li> <li>Docker Engine,</li> <li>Docker Registry account,</li> </ul>"},{"location":"#compatibility","title":"Compatibility","text":"<p>This workshop has been tested on the following platforms:</p> <ul> <li>Play with Docker,</li> <li>Docker Desktop: v2.2.0.5</li> <li>Engine: v19.03.8</li> <li>Compose: v1.25.4</li> <li>Docker for Linux:</li> <li>Client: v18.09.7</li> <li>Docker Engine: v19.03.5</li> </ul>"},{"location":"#technology-used","title":"Technology Used","text":"<ul> <li>Docker CLI,</li> <li>Docker Hub.</li> </ul>"},{"location":"#credits","title":"Credits","text":"<ul> <li>John Zaccone</li> <li>Jason Kennedy</li> <li>Steve Martinelli</li> <li>Remko de Knikker</li> </ul>"},{"location":"SUMMARY/","title":"Table of contents","text":"<ul> <li>Introduction</li> </ul>"},{"location":"SUMMARY/#getting-started","title":"Getting Started","text":"<ul> <li>Pre-work</li> </ul>"},{"location":"SUMMARY/#docker-101","title":"Docker 101","text":"<ul> <li>Lab 1</li> <li>Lab 2</li> <li>Lab 3</li> </ul>"},{"location":"SUMMARY/#resources","title":"Resources","text":"<ul> <li>IBM Developer</li> </ul>"},{"location":"lab-0/","title":"Lab 0 - Install Docker","text":""},{"location":"lab-0/#overview","title":"Overview","text":"<p>In this lab, you will install docker. We will be using docker throughout the rest of the labs.</p>"},{"location":"lab-0/#prerequisites","title":"Prerequisites","text":"<p>None</p>"},{"location":"lab-0/#use-theia-cloud-ide-with-docker","title":"Use Theia - Cloud IDE (With Docker)","text":"<ol> <li>Go to https://labs.cognitiveclass.ai ,</li> <li>Sign up for CognitiveClass.ai using your <code>IBM id</code> ,</li> <li>Login with your <code>IBM Id</code>,</li> <li>Select <code>Theia - Cloud IDE (With Docker)</code>,</li> <li>Select <code>Terminal</code> &gt; <code>New Terminal</code>.</li> </ol> <pre><code>docker version\n</code></pre>"},{"location":"lab-0/#optional-install-docker-desktop","title":"Optional: Install Docker Desktop","text":"<ol> <li> <p>Navigate to Get Docker,</p> </li> <li> <p>Select the option for your operating system or platform:</p> <ul> <li>Docker Desktop for Mac,</li> <li>Docker Desktop for Windows,</li> <li>Docker for Linux,</li> </ul> <p></p> </li> <li> <p>On this page you will find the installation for your operating systems. For example, if you are using a Mac, select \"MacOS\", to find the installation for the Mac platform.</p> </li> <li> <p>For Mac, you are re-directed to https://hub.docker.com/editions/community/docker-ce-desktop-mac where you can click the <code>Get Stable</code> edition,</p> <p></p> </li> </ol>"},{"location":"lab-0/#optional-use-play-with-docker","title":"Optional: Use play-with-docker","text":"<p>If you don't want to install Docker, an alternative is to use Play-With-Docker. Play-With-Docker is a website where you can run terminals directly from your browser that have Docker installed. All of the labs for this course can be run on Play-With-Docker, though we recommend installing docker locally on your host, so that you can continue your docker journey when this course has completed. To use Play-With-Docker, navigate to Play-With-Docker in your browser.</p>"},{"location":"lab-0/#summary","title":"Summary","text":"<p>By installing Docker, or alternatively, familiarizing yourself with Play with Docker, you are ready to complete the remaining labs in this course.</p>"},{"location":"lab-1/","title":"Lab 1 - Running Your First Container","text":""},{"location":"lab-1/#overview","title":"Overview","text":"<p>In this lab, you will run your first Docker container.</p> <p>Containers are just a process (or a group of processes) running in isolation. Isolation is achieved via linux namespaces, control groups (cgroups), seccomp and SELinux. Note that linux namespaces and control groups are built into the linux kernel! Other than the linux kernel itself, there is nothing special about containers.</p> <p>What makes containers useful is the tooling that surrounds it. For these labs, we will be using Docker, which has been a widely adopted tool for using containers to build applications. Docker provides developers and operators with a friendly interface to build, ship and run containers on any environment with a Docker engine. Because Docker client requires a Docker engine, an alternative is to use Podman, which is a deamonless container engine to develop, manage and run OCI containers and is able to run containers as root or in rootless mode. For those reasons, we recommend Podman but because of adoption, this lab still uses Docker.</p> <p>The first part of this lab, we will run our first container, and learn how to inspect it. We will be able to witness the namespace isolation that we acquire from the linux kernel.</p> <p>After we run our first container, we will dive into other uses of containers. You can find many examples of these on the Docker Store, and we will run several different types of containers on the same host. This will allow us to see the benefit of isolation- where we can run multiple containers on the same host without conflicts.</p> <p>We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation.</p>"},{"location":"lab-1/#prerequisites","title":"Prerequisites","text":"<p>Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from <code>Theia - Cloud IDE</code> at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example.</p>"},{"location":"lab-1/#get-started","title":"Get Started","text":"<p>Run <code>docker -h</code>,</p> <pre><code>$ docker -h\nFlag shorthand -h has been deprecated, please use --help\n\nUsage:  docker [OPTIONS] COMMAND\n\nA self-sufficient runtime for containers\n\n...\n\nManagement Commands:\n  builder     Manage builds\n  config      Manage Docker configs\n  container   Manage containers\n  engine      Manage the docker engine\n  image       Manage images\n  network     Manage networks\n  node        Manage Swarm nodes\n  plugin      Manage plugins\n  secret      Manage Docker secrets\n  service     Manage services\n  stack       Manage Docker stacks\n  swarm       Manage Swarm\n  system      Manage Docker\n  trust       Manage trust on Docker images\n  volume      Manage volumes\n</code></pre> <p>The Docker command line can be used to manage several features of the Docker Engine. In this lab, we will mainly focus on the <code>container</code> command.</p> <p>If <code>podman</code> is installed, you can run the alternative command for comparison.</p> <pre><code>sudo podman -h\n</code></pre> <p>You can additionally review the version of your Docker installation,</p> <pre><code>docker version\n\nClient:\n  Version:      19.03.6\n  ...\n\nServer: Docker Engine - Community\n  Engine\n    Version:    19.03.5\n    ...\n</code></pre> <p>You note that Docker installs both a <code>Client</code> and a <code>Server: Docker Engine</code>. For instance, if you run the same command for podman, you will see only a CLI version, because podman runs daemonless and relies on an OCI compliant container runtime (runc, crun, runv etc) to interface with the OS to create the running containers.</p> <pre><code>sudo podman version --events-backend=none\nVersion:      2.1.1\nAPI Version:  2.0.0\nGo Version:   go1.15.2\nBuilt:        Thu Jan  1 00:00:00 1970\nOS/Arch:      linux/amd64\n</code></pre>"},{"location":"lab-1/#step-1-run-your-first-container","title":"Step 1: Run your first container","text":"<p>We are going to use the Docker CLI to run our first container.</p> <ol> <li> <p>Open a terminal on your local computer</p> </li> <li> <p>Run <code>docker container run -t ubuntu top</code></p> <p>Use the <code>docker container run</code> command to run a container with the ubuntu image using the <code>top</code> command. The <code>-t</code> flags allocate a pseudo-TTY which we need for the <code>top</code> to work correctly.</p> <pre><code>$ docker container run -it ubuntu top\nUnable to find image 'ubuntu:latest' locally\nlatest: Pulling from library/ubuntu\naafe6b5e13de: Pull complete\n0a2b43a72660: Pull complete\n18bdd1e546d2: Pull complete\n8198342c3e05: Pull complete\nf56970a44fd4: Pull complete\nDigest: sha256:f3a61450ae43896c4332bda5e78b453f4a93179045f20c8181043b26b5e79028\nStatus: Downloaded newer image for ubuntu:latest\n</code></pre> <p>The <code>docker run</code> command will result first in a <code>docker pull</code> to download the ubuntu image onto your host. Once it is downloaded, it will start the container. The output for the running container should look like this:</p> <pre><code>top - 20:32:46 up 3 days, 17:40,  0 users,  load average: 0.00, 0.01, 0.00\nTasks:   1 total,   1 running,   0 sleeping,   0 stopped,   0 zombie\n%Cpu(s):  0.0 us,  0.1 sy,  0.0 ni, 99.9 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st\nKiB Mem :  2046768 total,   173308 free,   117248 used,  1756212 buff/cache\nKiB Swap:  1048572 total,  1048572 free,        0 used.  1548356 avail Mem\n\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND\n    1 root      20   0   36636   3072   2640 R   0.3  0.2   0:00.04 top\n</code></pre> <p><code>top</code> is a linux utility that prints the processes on a system and orders them by resource consumption. Notice that there is only a single process in this output: it is the <code>top</code> process itself. We don't see other processes from our host in this list because of the PID namespace isolation.</p> <p>Containers use linux namespaces to provide isolation of system resources from other containers or the host. The PID namespace provides isolation for process IDs. If you run <code>top</code> while inside the container, you will notice that it shows the processes within the PID namespace of the container, which is much different than what you can see if you ran <code>top</code> on the host.</p> <p>Even though we are using the <code>ubuntu</code> image, it is important to note that our container does not have its own kernel. Its uses the kernel of the host and the <code>ubuntu</code> image is used only to provide the file system and tools available on an ubuntu system.</p> </li> <li> <p>Inspect the container with <code>docker container exec</code></p> <p>The <code>docker container exec</code> command is a way to \"enter\" a running container's namespaces with a new process.</p> <p>Open a new terminal. On cognitiveclass.ai, select <code>Terminal</code> &gt; <code>New Terminal</code>.</p> <p>Using play-with-docker.com, to open a new terminal connected to node1, click \"Add New Instance\" on the lefthand side, then ssh from node2 into node1 using the IP that is listed by 'node1  '. For example:</p> <pre><code>[node2] (local) root@192.168.0.17 ~\n$ ssh 192.168.0.18\n[node1] (local) root@192.168.0.18 ~\n$\n</code></pre> <p>In the new terminal, use the <code>docker container ls</code> command to get the ID of the running container you just created.</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE                      COMMAND                  CREATED             STATUS                         PORTS                       NAMES\nb3ad2a23fab3        ubuntu                     \"top\"                    29 minutes ago      Up 29 minutes                                              goofy_nobel\n</code></pre> <p>Then use that id to run <code>bash</code> inside that container using the <code>docker container exec</code> command. Since we are using bash and want to interact with this container from our terminal, use <code>-it</code> flags to run using interactive mode while allocating a psuedo-terminal.</p> <pre><code>$ docker container exec -it b3ad2a23fab3 bash\nroot@b3ad2a23fab3:/#\n</code></pre> <p>And Voila! We just used the <code>docker container exec</code> command to \"enter\" our container's namespaces with our bash process. Using <code>docker container exec</code> with <code>bash</code> is a common pattern to inspect a docker container.</p> <p>Notice the change in the prefix of your terminal. e.g. <code>root@b3ad2a23fab3:/</code>. This is an indication that we are running bash \"inside\" of our container.</p> <p>Note: This is not the same as ssh'ing into a separate host or a VM. We don't need an ssh server to connect with a bash process. Remember that containers use kernel-level features to achieve isolation and that containers run on top of the kernel. Our container is just a group of processes running in isolation on the same host, and we can use <code>docker container exec</code> to enter that isolation with the <code>bash</code> process. After running <code>docker container exec</code>, the group of processes running in isolation (i.e. our container) include <code>top</code> and <code>bash</code>.</p> <p>From the same termina, run <code>ps -ef</code> to inspect the running processes.</p> <pre><code>root@b3ad2a23fab3:/# ps -ef\nUID        PID  PPID  C STIME TTY          TIME CMD\nroot         1     0  0 20:34 ?        00:00:00 top\nroot        17     0  0 21:06 ?        00:00:00 bash\nroot        27    17  0 21:14 ?        00:00:00 ps -ef\n</code></pre> <p>You should see only the <code>top</code> process, <code>bash</code> process and our <code>ps</code> process.</p> <p>For comparison, exit the container, and run <code>ps -ef</code> or <code>top</code> on the host. These commands will work on linux or mac. For windows, you can inspect the running processes using <code>tasklist</code>.</p> <pre><code>root@b3ad2a23fab3:/# exit\nexit\n$ ps -ef\n# Lots of processes!\n</code></pre> <p>Technical Deep Dive PID is just one of the linux namespaces that provides containers with isolation to system resources. Other linux namespaces include: - MNT - Mount and unmount directories without affecting other namespaces - NET - Containers have their own network stack - IPC - Isolated interprocess communication mechanisms such as message queues. - User - Isolated view of users on the system - UTC - Set hostname and domain name per container</p> <p>These namespaces together provide the isolation for containers that allow them to run together securely and without conflict with other containers running on the same system. Next, we will demonstrate different uses of containers. and the benefit of isolation as we run multiple containers on the same host.</p> <p>Note: Namespaces are a feature of the linux kernel. But Docker allows you to run containers on Windows and Mac... how does that work? The secret is that embedded in the Docker product or Docker engine is a linux subsystem. Docker open-sourced this linux subsystem to a new project: LinuxKit. Being able to run containers on many different platforms is one advantage of using the Docker tooling with containers.</p> <p>In addition to running linux containers on Windows using a linux subsystem, native Windows containers are now possible due the creation of container primitives on the Windows OS. Native Windows containers can be run on Windows 10 or Windows Server 2016 or newer.</p> <p>Note: if you run this exercise in a containerized terminal and execute the <code>ps -ef</code> command in the terminal, e.g. in <code>https://labs.cognitiveclass.ai</code>, you will still see a limited set of processes after exiting the <code>exec</code> command. You can try to run the <code>ps -ef</code> command in a terminal on your local machine to see all processes.</p> </li> <li> <p>Clean up the container running the <code>top</code> processes by typing: <code>&lt;ctrl&gt;-c</code>, list all containers and remove the containers by their id.</p> <pre><code>docker ps -a\n\ndocker rm &lt;CONTAINER ID&gt;\n</code></pre> </li> </ol>"},{"location":"lab-1/#step-2-run-multiple-containers","title":"Step 2: Run Multiple Containers","text":"<ol> <li> <p>Explore the Docker Hub</p> <p>The Docker Hub is the public central registry for Docker images, which contains community and official images.</p> <p>When searching for images you will find filters for \"Docker Certified\", \"Verified Publisher\" and \"Official Images\" images. Select the \"Docker Certified\" filter, to find images that are deemed enterprise-ready and are tested with Docker Enterprise Edition product. It is important to avoid using unverified content from the Docker Store when developing your own images that are intended to be deployed into the production environment. These unverified images may contain security vulnerabilities or possibly even malicious software.</p> <p>In Step 2 of this lab, we will start a couple of containers using some verified images from the Docker Hub: nginx web server, and mongo database.</p> </li> <li> <p>Run an Nginx server</p> <p>Let's run a container using the official Nginx image from the Docker Hub.</p> <pre><code>$ docker container run --detach --publish 8080:80 --name nginx nginx\nUnable to find image 'nginx:latest' locally\nlatest: Pulling from library/nginx\n36a46ebd5019: Pull complete\n57168433389f: Pull complete\n332ec8285c50: Pull complete\nDigest: sha256:c15f1fb8fd55c60c72f940a76da76a5fccce2fefa0dd9b17967b9e40b0355316\nStatus: Downloaded newer image for nginx:latest\n5e1bf0e6b926bd73a66f98b3cbe23d04189c16a43d55dd46b8486359f6fdf048\n</code></pre> <p>We are using a couple of new flags here. The <code>--detach</code> flag will run this container in the background. The <code>publish</code> flag publishes port 80 in the container (the default port for nginx), via port 8080 on our host. Remember that the NET namespace gives processes of the container their own network stack. The <code>--publish</code> flag is a feature that allows us to expose networking through the container onto the host.</p> <p>How do you know port 80 is the default port for nginx? Because it is listed in the documentation on the Docker Hub. In general, the documentation for the verified images is very good, and you will want to refer to them when running containers using those images.</p> <p>We are also specifying the <code>--name</code> flag, which names the container. Every container has a name, if you don't specify one, Docker will randomly assign one for you. Specifying your own name makes it easier to run subsequent commands on your container since you can reference the name instead of the id of the container. For example: <code>docker container inspect nginx</code> instead of <code>docker container inspect 5e1</code>.</p> <p>Since this is the first time you are running the nginx container, it will pull down the nginx image from the Docker Store. Subsequent containers created from the Nginx image will use the existing image located on your host.</p> <p>Nginx is a lightweight web server. You can access it on port 8080 on your localhost.</p> </li> <li> <p>Access the nginx server on localhost:8080.</p> <pre><code>curl localhost:8080\n</code></pre> <p>will return the HTML home page of Nginx,</p> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n&lt;title&gt;Welcome to nginx!&lt;/title&gt;\n&lt;style&gt;\n    body {\n        width: 35em;\n        margin: 0 auto;\n        font-family: Tahoma, Verdana, Arial, sans-serif;\n    }\n&lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;\n</code></pre> </li> <li> <p>If you are using play-with-docker, look for the <code>8080</code> link near the top of the page, or if you run a Docker client with access to a local browser,</p> <p></p> </li> <li> <p>Run a mongo DB server</p> <p>Now, run a mongoDB server. We will use the official mongoDB image from the Docker Hub. Instead of using the <code>latest</code> tag (which is the default if no tag is specified), we will use a specific version of the mongo image: 4.4.</p> <pre><code>$ docker container run --detach --publish 8081:27017 --name mongo mongo:4.4\nUnable to find image mongo:4.4 locally\n4.4: Pulling from library/mongo\nd13d02fa248d: Already exists\nbc8e2652ce92: Pull complete\n3cc856886986: Pull complete\nc319e9ec4517: Pull complete\nb4cbf8808f94: Pull complete\ncb98a53e6676: Pull complete\nf0485050cd8a: Pull complete\nac36cdc414b3: Pull complete\n61814e3c487b: Pull complete\n523a9f1da6b9: Pull complete\n3b4beaef77a2: Pull complete\nDigest: sha256:d13c897516e497e898c229e2467f4953314b63e48d4990d3215d876ef9d1fc7c\nStatus: Downloaded newer image for mongo:4.4\nd8f614a4969fb1229f538e171850512f10f490cb1a96fca27e4aa89ac082eba5\n</code></pre> <p>Again, since this is the first time we are running a mongo container, we will pull down the mongo image from the Docker Store. We are using the <code>--publish</code> flag to expose the 27017 mongo port on our host. We have to use a port other than 8080 for the host mapping, since that port is already exposed on our host. Again refer to the official docs on the Docker Hub to get more details about using the mongo image.</p> </li> <li> <p>Access localhost:8081 to see some output from mongo.</p> <pre><code>curl localhost:8081\n</code></pre> <p>which will return a warning from MongoDB,</p> <pre><code>It looks like you are trying to access MongoDB over HTTP on the native driver port.\n</code></pre> </li> <li> <p>If you are using play-with-docker, look for the <code>8080</code> link near the top of the page.</p> <p></p> </li> <li> <p>Check your running containers with <code>docker container ls</code></p> <pre><code>$ docker container ls\nCONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES\nd6777df89fea    nginx    \"nginx -g 'daemon ...\"    Less than a second ago    Up 2 seconds    0.0.0.0:8080-&gt;80/tcp    nginx\nead80a0db505    mongo    \"docker-entrypoint...\"    17 seconds ago    Up 19 seconds    0.0.0.0:8081-&gt;27017/tcp    mongo\naf549dccd5cf    ubuntu    \"top\"    5 minutes ago    Up 5 minutes    priceless_kepler\n</code></pre> <p>You should see that you have an Nginx web server container, and a MongoDB container running on your host. Note that we have not configured these containers to talk to each other.</p> <p>You can see the \"nginx\" and \"mongo\" names that we gave to our containers, and the random name (in my case \"priceless_kepler\") that was generated for the ubuntu container. You can also see that the port mappings that we specified with the <code>--publish</code> flag. For more details information on these running containers you can use the <code>docker container inspect [container id</code> command.</p> <p>One thing you might notice is that the mongo container is running the <code>docker-entrypoint</code> command. This is the name of the executable that is run when the container is started. The mongo image requires some prior configuration before kicking off the DB process. You can see exactly what the script does by looking at it on github. Typically, you can find the link to the github source from the image description page on the Docker Store website.</p> <p>Containers are self-contained and isolated, which means we can avoid potential conflicts between containers with different system or runtime dependencies. For example: deploying an app that uses Java 7 and another app that uses Java 8 on the same host. Or running multiple nginx containers that all have port 80 as their default listening ports (if exposing on the host using the <code>--publish</code> flag, the ports selected for the host will need to be unique). Isolation benefits are possible because of Linux Namespaces.</p> <p>Note: You didn't have to install anything on your host (other than Docker) to run these processes! Each container includes the dependencies that it needs within the container, so you don't need to install anything on your host directly.</p> <p>Running multiple containers on the same host gives us the ability to fully utilize the resources (cpu, memory, etc) available on single host. This can result in huge cost savings for an enterprise.</p> <p>While running images directly from the Docker Hub can be useful at times, it is more useful to create custom images, and refer to official images as the starting point for these images. We will dive into building our own custom images in Lab 2.</p> </li> </ol>"},{"location":"lab-1/#step-3-clean-up","title":"Step 3: Clean Up","text":"<p>Completing this lab results in a bunch of running containers on your host. Let's clean these up.</p> <ol> <li> <p>First get a list of the containers running using <code>docker container ls</code>.</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE               COMMAND                  CREATED             STATUS              PORTS                     NAMES\nd6777df89fea        nginx               \"nginx -g 'daemon ...\"   3 minutes ago       Up 3 minutes        0.0.0.0:8080-&gt;80/tcp      nginx\nead80a0db505        mongo               \"docker-entrypoint...\"   3 minutes ago       Up 3 minutes        0.0.0.0:8081-&gt;27017/tcp   mongo\naf549dccd5cf        ubuntu              \"top\"                    8 minutes ago       Up 8 minutes                                  priceless_kepler\n</code></pre> </li> <li> <p>Next,  run <code>docker container stop [container id]</code> for each container in the list. You can also use the names of the containers that you specified before.</p> <pre><code>$ docker container stop d67 ead af5\nd67\nead\naf5\n</code></pre> <p>Note: You only have to reference enough digits of the ID to be unique. Three digits is almost always enough.</p> </li> <li> <p>Remove the stopped containers</p> <p><code>docker system prune</code> is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images.</p> <pre><code>$ docker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all volumes not used by at least one container\n        - all networks not used by at least one container\n        - all dangling images\nAre you sure you want to continue? [y/N] y\nDeleted Containers:\n7872fd96ea4695795c41150a06067d605f69702dbcb9ce49492c9029f0e1b44b\n60abd5ee65b1e2732ddc02b971a86e22de1c1c446dab165462a08b037ef7835c\n31617fdd8e5f584c51ce182757e24a1c9620257027665c20be75aa3ab6591740\n\nTotal reclaimed space: 12B\n</code></pre> </li> </ol>"},{"location":"lab-1/#summary","title":"Summary","text":"<p>In this lab, you created your first Ubuntu, Nginx and MongoDB containers.</p> <p>Key Takeaways</p> <ul> <li>Containers are composed of linux namespaces and control groups that provide isolation from other containers and the host.</li> <li>Because of the isolation properties of containers, you can schedule many containers on a single host without worrying about conflicting dependencies. This makes it easier to run multiple containers on a single host: fully utilizing resources allocated to that host, and ultimately saving some money on server costs.</li> <li>Avoid using unverified content from the Docker Store when developing your own images because these images may contain security vulnerabilities or possibly even malicious software.</li> <li>Containers include everything they need to run the processes within them, so there is no need to install additional dependencies directly on your host.</li> </ul>"},{"location":"lab-2/","title":"Lab 2 - Adding Value with Custom Docker Images","text":""},{"location":"lab-2/#overview","title":"Overview","text":"<p>In this lab, we build on our knowledge from lab 1 where we used Docker commands to run containers. We will create a custom Docker Image built from a Dockerfile. Once we build the image, we will push it to a central registry where it can be pulled to be deployed on other environments. Also, we will briefly describe image layers, and how Docker incorporates \"copy-on-write\" and the union file system to efficiently store images and run containers.</p> <p>We will be using a few Docker commands in this lab. For full documentation on available commands check out the official documentation.</p>"},{"location":"lab-2/#prerequisites","title":"Prerequisites","text":"<p>Completed Lab 0: You must have access to a docker client, either on localhost, use a terminal from <code>Theia - Cloud IDE</code> at https://labs.cognitiveclass.ai/tools/theiadocker or be using Play with Docker for example.</p>"},{"location":"lab-2/#step-1-create-a-python-app-without-using-docker","title":"Step 1: Create a python app (without using Docker)","text":"<p>Run the following command to create a file named <code>app.py</code> with a simple python program. (copy-paste the entire code block)</p> <pre><code>echo 'from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"hello world!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")' &gt; app.py\n</code></pre> <p>This is a simple python app that uses flask to expose a http web server on port 5000 (5000 is the default port for flask). Don't worry if you are not too familiar with python or flask, these concepts can be applied to an application written in any language.</p> <p>Optional: If you have python and pip installed, you can run this app locally. If not, move on to the next step.</p> <pre><code>$ python3 --version\nPython 3.6.9\n$ pip3 --version\npip 9.0.1 from /usr/lib/python3/dist-packages (python 3.6)\n$ pip3 install flask\nCollecting flask\n  Downloading https://files.pythonhosted.org/packages/f2/28/2a03252dfb9ebf377f40fba6a7841b47083260bf8bd8e737b0c6952df83f/Flask-1.1.2-py2.py3-none-any.whl (94kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 102kB 6.2MB/s\nCollecting Werkzeug&gt;=0.15 (from flask)\n  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 307kB 3.4MB/s\nCollecting itsdangerous&gt;=0.24 (from flask)\n  Downloading https://files.pythonhosted.org/packages/76/ae/44b03b253d6fade317f32c24d100b3b35c2239807046a4c953c7b89fa49e/itsdangerous-1.1.0-py2.py3-none-any.whl\nCollecting click&gt;=5.1 (from flask)\n  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 92kB 10.2MB/s\nCollecting Jinja2&gt;=2.10.1 (from flask)\n  Downloading https://files.pythonhosted.org/packages/30/9e/f663a2aa66a09d838042ae1a2c5659828bb9b41ea3a6efa20a20fd92b121/Jinja2-2.11.2-py2.py3-none-any.whl (125kB)\n    100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 133kB 8.5MB/s\nCollecting MarkupSafe&gt;=0.23 (from Jinja2&gt;=2.10.1-&gt;flask)\n  Downloading https://files.pythonhosted.org/packages/b2/5f/23e0023be6bb885d00ffbefad2942bc51a620328ee910f64abe5a8d18dd1/MarkupSafe-1.1.1-cp36-cp36m-manylinux1_x86_64.whl\nInstalling collected packages: Werkzeug, itsdangerous, click, MarkupSafe, Jinja2, flask\nSuccessfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0\n\n$ python3 app.py\n * Serving Flask app \"app\" (lazy loading)\n * Environment: production\n   WARNING: This is a development server. Do not use it in a production deployment.\n   Use a production WSGI server instead.\n * Debug mode: off\n * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n</code></pre>"},{"location":"lab-2/#step-2-create-and-build-the-docker-image","title":"Step 2: Create and build the Docker Image","text":"<p>Now, what if you don't have python installed locally? Don't worry! Because you don't need it. One of the advantages of using containers is that you can build python inside your containers, without having python installed on your host machine.</p> <ol> <li> <p>Create a <code>Dockerfile</code> but running the following command. (copy-paste the entire code block)</p> <pre><code>echo 'FROM python:3.8-alpine\nRUN pip install flask\nCMD [\"python\",\"app.py\"]\nCOPY app.py /app.py' &gt; Dockerfile\n</code></pre> <p>A Dockerfile lists the instructions needed to build a docker image. Let's go through the above file line by line.</p> <p>FROM python:3.8-alpine This is the starting point for your Dockerfile. Every Dockerfile must start with a <code>FROM</code> line that is the starting image to build your layers on top of.</p> <p>In this case, we are selecting the <code>python:3.8-alpine</code> base layer (see Dockerfile for python3.8/alpine3.12) since it already has the version of python and pip that we need to run our application.</p> <p>The <code>alpine</code> version means that it uses the Alpine Linux distribution, which is significantly smaller than many alternative flavors of Linux, around 8 MB in size, while a minimal installation to disk might be around 130 MB. A smaller image means it will download (deploy) much faster, and it also has advantages for security because it has a smaller attack surface. Alpine Linux is a Linux distribution based on musl and BusyBox.</p> <p>Here we are using the \"3.8-alpine\" tag for the python image. Take a look at the available tags for the official python image on the Docker Hub. It is best practice to use a specific tag when inheriting a parent image so that changes to the parent dependency are controlled. If no tag is specified, the \"latest\" tag takes into effect, which is acts as a dynamic pointer that points to the latest version of an image.</p> <p>For security reasons, it is very important to understand the layers that you build your docker image on top of. For that reason, it is highly recommended to only use \"official\" images found in the docker hub, or non-community images found in the docker-store. These images are vetted to meet certain security requirements, and also have very good documentation for users to follow. You can find more information about this python base image, as well as all other images that you can use, on the docker hub.</p> <p>For a more complex application you may find the need to use a<code>FROM</code> image that is higher up the chain. For example, the parent Dockerfile for our python app  starts with <code>FROM alpine</code>, then specifies a series of <code>CMD</code> and <code>RUN</code> commands for the image. If you needed more fine-grained control, you could start with <code>FROM alpine</code> (or a different distribution) and run those steps yourself. To start off though, I recommend using an official image that closely matches your needs.</p> <p>RUN pip install flask The <code>RUN</code> command executes commands needed to set up your image for your application, such as installing packages, editing files, or changing file permissions. In this case we are installing flask. The <code>RUN</code> commands are executed at build time, and are added to the layers of your image.</p> <p>CMD [\"python\",\"app.py\"] <code>CMD</code> is the command that is executed when you start a container. Here we are using <code>CMD</code> to run our python app.</p> <p>There can be only one <code>CMD</code> per Dockerfile. If you specify more thane one <code>CMD</code>, then the last <code>CMD</code> will take effect. The parent python:3.8-alpine also specifies a <code>CMD</code> (<code>CMD python3</code>). You can find the Dockerfile for the official python:alpine image here.</p> <p>You can use the official python image directly to run python scripts without installing python on your host. But today, we are creating a custom image to include our source, so that we can build an image with our application and ship it around to other environments.</p> <p>COPY app.py /app.py This copies the app.py in the local directory (where you will run <code>docker image build</code>) into a new layer of the image. This instruction is the last line in the Dockerfile. Layers that change frequently, such as copying source code into the image, should be placed near the bottom of the file to take full advantage of the Docker layer cache. This allows us to avoid rebuilding layers that could otherwise be cached. For instance, if there was a change in the <code>FROM</code> instruction, it would invalidate the cache for all subsequent layers of this image. We will demonstrate a this little later in this lab.</p> <p>It seems counter-intuitive to put this after the <code>CMD [\"python\",\"app.py\"]</code> line. Remember, the <code>CMD</code> line is executed only when the container is started, so we won't get a <code>file not found</code> error here.</p> <p>And there you have it: a very simple Dockerfile. A full list of commands you can put into a Dockerfile can be found here. Now that we defined our Dockerfile, let's use it to build our custom docker image.</p> </li> <li> <p>Build the docker image.</p> <p>Pass in <code>-t</code> to name your image <code>python-hello-world</code>.</p> <pre><code>$  docker image build -t python-hello-world .\nSending build context to Docker daemon  3.072kB\nStep 1/4 : FROM python:3.8-alpine\n3.8-alpine: Pulling from library/python\ndf20fa9351a1: Pull complete\n36b3adc4ff6f: Pull complete\n3e7ef1bb9eba: Pull complete\n78538f72d6a9: Pull complete\n07bc731e0055: Pull complete\nDigest: sha256:cbc08bfc4b1b732076742f52852ede090e960ab7470d0a60ee4f964cfa7c710a\nStatus: Downloaded newer image for python:3.8-alpine\n---&gt; 0f03316d4a27\nStep 2/4 : RUN pip install flask\n---&gt; Running in 1454bdd1ea98\nCollecting flask\nDownloading Flask-1.1.2-py2.py3-none-any.whl (94 kB)\nCollecting itsdangerous&gt;=0.24\nDownloading itsdangerous-1.1.0-py2.py3-none-any.whl (16 kB)\nCollecting Werkzeug&gt;=0.15\nDownloading Werkzeug-1.0.1-py2.py3-none-any.whl (298 kB)\nCollecting click&gt;=5.1\nDownloading click-7.1.2-py2.py3-none-any.whl (82 kB)\nCollecting Jinja2&gt;=2.10.1\nDownloading Jinja2-2.11.2-py2.py3-none-any.whl (125 kB)\nCollecting MarkupSafe&gt;=0.23\nDownloading MarkupSafe-1.1.1.tar.gz (19 kB)\nBuilding wheels for collected packages: MarkupSafe\nBuilding wheel for MarkupSafe (setup.py): started\nBuilding wheel for MarkupSafe (setup.py): finished with status 'done'\nCreated wheel for MarkupSafe: filename=MarkupSafe-1.1.1-py3-none-any.whl size=12627 sha256=155e3314602dfac3c8ea245edc217c235afb4c818932574d6d61529ef0c14ea4\nStored in directory: /root/.cache/pip/wheels/0c/61/d6/4db4f4c28254856e82305fdb1f752ed7f8482e54c384d8cb0e\nSuccessfully built MarkupSafe\nInstalling collected packages: itsdangerous, Werkzeug, click, MarkupSafe, Jinja2, flask\nSuccessfully installed Jinja2-2.11.2 MarkupSafe-1.1.1 Werkzeug-1.0.1 click-7.1.2 flask-1.1.2 itsdangerous-1.1.0\nRemoving intermediate container 1454bdd1ea98\n---&gt; 97d747fc7771\nStep 3/4 : CMD [\"python\",\"app.py\"]\n---&gt; Running in e2bf74801c81\nRemoving intermediate container e2bf74801c81\n---&gt; d5adbccf5116\nStep 4/4 : COPY app.py /app.py\n---&gt; 3c24958f29d3\nSuccessfully built 3c24958f29d3\nSuccessfully tagged python-hello-world:latest\n</code></pre> <p>Verify that your image shows up in your image list via <code>docker image ls</code>.</p> <pre><code>$ docker image ls\nREPOSITORY    TAG    IMAGE ID    CREATED    SIZE\npython-hello-world   latest    3c24958f29d3    52 seconds ago      53.4MB\npython    3.8-alpine    0f03316d4a27    2 weeks ago    42.7MB\n</code></pre> <p>Note that your base image <code>python:3.8-alpine</code> is also in your list.</p> </li> <li> <p>You can run a history command to show the history of an image and its layers,</p> <pre><code>docker history python-hello-world\ndocker history python:3.8-alpine\n</code></pre> </li> </ol>"},{"location":"lab-2/#step-3-run-the-docker-image","title":"Step 3: Run the Docker image","text":"<p>Now that you have built the image, you can run it to see that it works.</p> <ol> <li> <p>Run the Docker image</p> <pre><code>$ docker run -p 5001:5000 -d python-hello-world\n0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26\n</code></pre> <p>The <code>-p</code> flag maps a port running inside the container to your host. In this case, we are mapping the python app running on port 5000 inside the container, to port 5001 on your host. Note that if port 5001 is already in use by another application on your host, you may have to replace 5001 with another value, such as 5002.</p> </li> <li> <p>Navigate to localhost:5001 in a browser to see the results.</p> <p>In a terminal run <code>curl localhost:5001</code>, which returns <code>hello world!</code>.</p> <p>If you are using katacoda, click on the link in the left-hand pane that says: <code>View port at https://....environments.katacoda.com</code> then type in 5001 and click <code>Display Port</code>.</p> <p>In play-with-docker, click the link <code>5001</code> that should appear near the top of your session.</p> <p>You should see \"hello world!\" on your browser.</p> </li> <li> <p>Check the log output of the container.</p> <p>If you want to see logs from your application you can use the <code>docker container logs</code> command. By default, <code>docker container logs</code> prints out what is sent to standard out by your application. Use <code>docker container ls</code> to find the id for your running container.</p> <p><code>sh $ docker container ls CONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES 7b04d5320cb4    python-hello-world   \"python app.py\"     About a minute ago   Up About a minute   0.0.0.0:5001-&gt;5000/tcp   elastic_ganguly $ docker container logs [container id] * Serving Flask app \"app\" (lazy loading) * Environment: production   WARNING: This is a development server. Do not use it in a production deployment.   Use a production WSGI server instead. * Debug mode: off * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)    172.17.0.1 - - [23/Sep/2020 22:00:33] \"GET / HTTP/1.1\" 200 -</code></p> <p>The Dockerfile is how you create reproducible builds for your application. A common workflow is to have your CI/CD automation run <code>docker image build</code> as part of its build process. Once images are built, they will be sent to a central registry, where it can be accessed by all environments (such as a test environment) that need to run instances of that application. In the next step, we will push our custom image to the public docker registry: the docker hub, where it can be consumed by other developers and operators.</p> </li> </ol>"},{"location":"lab-2/#step-4-push-to-a-central-registry","title":"Step 4: Push to a central registry","text":"<ol> <li> <p>Navigate to Docker Hub and create an account if you haven't already. Alternatively, you can also use https://quay.io for instance.</p> <p>For this lab we will be using the docker hub as our central registry. Docker hub is a free service to store publicly available images, or you can pay to store private images. Go to the Docker Hub website and create a free account.</p> <p>Most organizations that use docker heavily will set up their own registry internally. To simplify things, we will be using the Docker Hub, but the following concepts apply to any registry.</p> </li> <li> <p>Login</p> <p>You can log into the image registry account by typing <code>docker login</code> on your terminal, or if using podman, type <code>podman login</code>.</p> <pre><code>$ export DOCKERHUB_USERNAME=&lt;dockerhub-username&gt;\n$ docker login docker.io -u $DOCKERHUB_USERNAME\npassword:\nWARNING! Your password will be stored unencrypted in /home/theia/.docker/config.json.\nConfigure a credential helper to remove this warning. See\nhttps://docs.docker.com/engine/reference/commandline/login/#credentials-store\n\nLogin Succeeded\n</code></pre> </li> <li> <p>Tag your image with your username</p> <p>The Docker Hub naming convention is to tag your image with [dockerhub username]/[image name]. To do this, we are going to tag our previously created image <code>python-hello-world</code> to fit that format.</p> <pre><code>docker tag python-hello-world $DOCKERHUB_USERNAME/python-hello-world\n</code></pre> </li> <li> <p>Push your image to the registry</p> <p>Once we have a properly tagged image, we can use the <code>docker push</code> command to push our image to the Docker Hub registry.</p> <pre><code>$ docker push $DOCKERHUB_USERNAME/python-hello-world\nThe push refers to a repository [docker.io/jzaccone/python-hello-world]\n2bce026769ac: Pushed\n64d445ecbe93: Pushed\n18b27eac38a1: Mounted from library/python\n3f6f25cd8b1e: Mounted from library/python\nb7af9d602a0f: Mounted from library/python\ned06208397d5: Mounted from library/python\n5accac14015f: Mounted from library/python\nlatest: digest: sha256:508238f264616bf7bf962019d1a3826f8487ed6a48b80bf41fd3996c7175fd0f size: 1786\n</code></pre> </li> <li> <p>Check out your image on docker hub in your browser</p> <p>Navigate to Docker Hub and go to your profile to see your newly uploaded image at <code>https://hub.docker.com/repository/docker/&lt;dockerhub-username&gt;/python-hello-world</code>.</p> <p>Now that your image is on Docker Hub, other developers and operations can use the <code>docker pull</code> command to deploy your image to other environments.</p> <p>Note: Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to. We also don't have to go through additional steps to provision these environments. Just one step: install docker, and you are good to go.</p> </li> </ol>"},{"location":"lab-2/#step-5-deploying-a-change","title":"Step 5: Deploying a Change","text":"<p>The \"hello world!\" application is overrated, let's update the app so that it says \"Hello Beautiful World!\" instead.</p> <ol> <li> <p>Update <code>app.py</code></p> <p>Replace the string \"Hello World\" with \"Hello Beautiful World!\" in <code>app.py</code>. You can update the file with the following command. (copy-paste the entire code block)</p> <pre><code>echo 'from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    return \"hello beautiful world!\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\")' &gt; app.py\n</code></pre> </li> <li> <p>Rebuild and push your image</p> <p>Now that your app is updated, you need repeat the steps above to rebuild your app and push it to the Docker Hub registry.</p> <p>First rebuild, this time use your Docker Hub username in the build command:</p> <pre><code>$  docker image build -t $DOCKERHUB_USERNAME/python-hello-world .\nSending build context to Docker daemon  3.072kB\nStep 1/4 : FROM python:3.6.1-alpine\n---&gt; c86415c03c37\nStep 2/4 : RUN pip install flask\n---&gt; Using cache\n---&gt; ce41f2517c16\nStep 3/4 : CMD python app.py\n---&gt; Using cache\n---&gt; 0ab91286958b\nStep 4/4 : COPY app.py /app.py\n---&gt; 3e08b2eeace1\nRemoving intermediate container 23a955e881fc\nSuccessfully built 3e08b2eeace1\nSuccessfully tagged &lt;dockerhub-username&gt;/python-hello-world:latest\n</code></pre> <p>Notice the \"Using cache\" for steps 1-3. These layers of the Docker Image have already been built and <code>docker image build</code> will use these layers from the cache instead of rebuilding them.</p> <pre><code>$ docker push $DOCKERHUB_USERNAME/python-hello-world\nThe push refers to a repository [docker.io/&lt;dockerhub-username&gt;/python-hello-world]\n94525867566e: Pushed\n64d445ecbe93: Layer already exists\n18b27eac38a1: Layer already exists\n3f6f25cd8b1e: Layer already exists\nb7af9d602a0f: Layer already exists\ned06208397d5: Layer already exists\n5accac14015f: Layer already exists\nlatest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786\n</code></pre> <p>There is a caching mechanism in place for pushing layers too. Docker Hub already has all but one of the layers from an earlier push, so it only pushes the one layer that has changed.</p> <p>When you change a layer, every layer built on top of that will have to be rebuilt. Each line in a Dockerfile builds a new layer that is built on the layer created from the lines before it. This is why the order of the lines in our Dockerfile is important. We optimized our Dockerfile so that the layer that is most likely to change (<code>COPY app.py /app.py</code>) is the last line of the Dockerfile. Generally for an application, your code changes at the most frequent rate. This optimization is particularly important for CI/CD processes, where you want your automation to run as fast as possible.</p> </li> </ol>"},{"location":"lab-2/#step-6-understanding-image-layers","title":"Step 6: Understanding Image Layers","text":"<p>One of the major design properties of Docker is its use of the union file system.</p> <p>Consider the Dockerfile that we created before:</p> <pre><code>FROM python:3.8-alpine\nRUN pip install flask\nCMD [\"python\",\"app.py\"]\nCOPY app.py /app.py\n</code></pre> <p>Each of these lines is a layer. Each layer contains only the delta, diff or changes from the layers before it. To put these layers together into a single running container, Docker makes use of the <code>union file system</code> to overlay layers transparently into a single view.</p> <p>Each layer of the image is <code>read-only</code>, except for the very top layer which is created for the running container. The read/write container layer implements \"copy-on-write\" which means that files that are stored in lower image layers are pulled up to the read/write container layer only when edits are being made to those files. Those changes are then stored in the running container layer. The \"copy-on-write\" function is very fast, and in almost all cases, does not have a noticeable effect on performance. You can inspect which files have been pulled up to the container level with the <code>docker diff</code> command. More information about how to use <code>docker diff</code> can be found here.</p> <p></p> <p>Since image layers are <code>read-only</code>, they can be shared by images and by running containers. For instance, creating a new python app with its own Dockerfile with similar base layers, would share all the layers that it had in common with the first python app.</p> <pre><code>FROM python:3.8-alpine\nRUN pip install flask\nCMD [\"python\",\"app2.py\"]\nCOPY app2.py /app2.py\n</code></pre> <p></p> <p>You can also experience the sharing of layers when you start multiple containers from the same image. Since the containers use the same read-only layers, you can imagine that starting up containers is very fast and has a very low footprint on the host.</p> <p>You may notice that there are duplicate lines in this Dockerfile and the Dockerfile you created earlier in this lab. Although this is a very trivial example, you can pull common lines of both Dockerfiles into a \"base\" Dockerfile, that you can then point to with each of your child Dockerfiles using the <code>FROM</code> command.</p> <p>Image layering enables the docker caching mechanism for builds and pushes. For example, the output for your last <code>docker push</code> shows that some of the layers of your image already exists on the Docker Hub.</p> <pre><code>$ docker push $DOCKERHUB_USERNAME/python-hello-world\nThe push refers to a repository [docker.io/&lt;dockerhub-username&gt;/python-hello-world]\n94525867566e: Pushed\n64d445ecbe93: Layer already exists\n18b27eac38a1: Layer already exists\n3f6f25cd8b1e: Layer already exists\nb7af9d602a0f: Layer already exists\ned06208397d5: Layer already exists\n5accac14015f: Layer already exists\nlatest: digest: sha256:91874e88c14f217b4cab1dd5510da307bf7d9364bd39860c9cc8688573ab1a3a size: 1786\n</code></pre> <p>To look more closely at layers, you can use the <code>docker image history</code> command of the python image we created.</p> <pre><code>$ docker image history python-hello-world\nIMAGE               CREATED             CREATED BY                                      SIZE                COMMENT\n3c24958f29d3        17 minutes ago      /bin/sh -c #(nop) COPY file:5fef1b9a6220c0e3\u2026   159B\nd5adbccf5116        17 minutes ago      /bin/sh -c #(nop)  CMD [\"python\" \"app.py\"]      0B\n97d747fc7771        17 minutes ago      /bin/sh -c pip install flask                    10.7MB\n0f03316d4a27        2 weeks ago         /bin/sh -c #(nop)  CMD [\"python3\"]              0B\n&lt;missing&gt;           2 weeks ago         /bin/sh -c set -ex;   wget -O get-pip.py \"$P\u2026   7.24MB\n&lt;missing&gt;           2 weeks ago         /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_SHA256\u2026   0B\n&lt;missing&gt;           2 weeks ago         /bin/sh -c #(nop)  ENV PYTHON_GET_PIP_URL=ht\u2026   0B\n&lt;missing&gt;           2 weeks ago         /bin/sh -c #(nop)  ENV PYTHON_PIP_VERSION=20\u2026   0B\n&lt;missing&gt;           7 weeks ago         /bin/sh -c cd /usr/local/bin  &amp;&amp; ln -s idle3\u2026   32B\n&lt;missing&gt;           7 weeks ago         /bin/sh -c set -ex  &amp;&amp; apk add --no-cache --\u2026   29.3MB\n&lt;missing&gt;           2 months ago        /bin/sh -c #(nop)  ENV PYTHON_VERSION=3.8.5     0B\n&lt;missing&gt;           3 months ago        /bin/sh -c #(nop)  ENV GPG_KEY=E3FF2839C048B\u2026   0B\n&lt;missing&gt;           3 months ago        /bin/sh -c apk add --no-cache ca-certificates   512kB\n&lt;missing&gt;           3 months ago        /bin/sh -c #(nop)  ENV LANG=C.UTF-8             0B\n&lt;missing&gt;           3 months ago        /bin/sh -c #(nop)  ENV PATH=/usr/local/bin:/\u2026   0B\n&lt;missing&gt;           3 months ago        /bin/sh -c #(nop)  CMD [\"/bin/sh\"]              0B\n&lt;missing&gt;           3 months ago        /bin/sh -c #(nop) ADD file:c92c248239f8c7b9b\u2026   5.57MB\n</code></pre> <p>Each line represents a layer of the image. You'll notice that the top lines match to your Dockerfile that you created, and the lines below are pulled from the parent python image. Don't worry about the \"\\&lt;missing&gt;\" tags. These are still normal layers; they have just not been given an ID by the docker system.</p>"},{"location":"lab-2/#step-7-clean-up","title":"Step 7: Clean up","text":"<p>Completing this lab results in a bunch of running containers on your host. Let's clean these up.</p> <ol> <li> <p>Run <code>docker container stop [container id]</code> for each container that is running</p> <p>First get a list of the containers running using <code>docker container ls</code>.</p> <pre><code>$ docker container ls\nCONTAINER ID        IMAGE                COMMAND             CREATED             STATUS              PORTS                    NAMES\n0b2ba61df37f        python-hello-world   \"python app.py\"     7 minutes ago       Up 7 minutes        0.0.0.0:5001-&gt;5000/tcp   practical_kirch\n</code></pre> <p>Then run <code>docker container stop [container id]</code> for each container in the list.</p> <pre><code>$ docker container stop 0b2\n0b2\n</code></pre> </li> <li> <p>Remove the stopped containers</p> <p><code>docker system prune</code> is a really handy command to clean up your system. It will remove any stopped containers, unused volumes and networks, and dangling images.</p> <pre><code>$ docker system prune\nWARNING! This will remove:\n        - all stopped containers\n        - all volumes not used by at least one container\n        - all networks not used by at least one container\n        - all dangling images\nAre you sure you want to continue? [y/N] y\nDeleted Containers:\n0b2ba61df37fb4038d9ae5d145740c63c2c211ae2729fc27dc01b82b5aaafa26\n\nTotal reclaimed space: 300.3kB\n</code></pre> </li> </ol>"},{"location":"lab-2/#summary","title":"Summary","text":"<p>In this lab, you started adding value by creating your own custom docker containers.</p> <p>Key Takeaways:</p> <ul> <li>The Dockerfile is how you create reproducible builds for your application and how you integrate your application with Docker into the CI/CD pipeline</li> <li>Docker images can be made available to all of your environments through a central registry. The Docker Hub is one example of a registry, but you can deploy your own registry on servers you control.</li> <li>Docker images contain all the dependencies that it needs to run an application within the image. This is useful because we no longer have deal with environment drift (version differences) when we rely on dependencies that are install on every environment we deploy to.</li> <li>Docker makes use of the union file system and \"copy on write\" to reuse layers of images. This lowers the footprint of storing images and significantly increases the performance of starting containers.</li> <li>Image layers are cached by the Docker build and push system. No need to rebuild or repush image layers that are already present on the desired system.</li> <li>Each line in a Dockerfile creates a new layer, and because of the layer cache, the lines that change more frequently (e.g. adding source code to an image) should be listed near the bottom of the file.</li> </ul>"},{"location":"lab-3/","title":"Lab 3 - Managing Data in Containers","text":""},{"location":"lab-3/#overview","title":"Overview","text":"<p>By default all files created inside a container are stored on a writable container layer. That means that:</p> <ul> <li>If the container no longer exists, the data is lost,</li> <li>The container's writable layer is tightly coupled to the host machine, and</li> <li>To manage the file system, you need a storage driver that provides a union file system, using the Linux kernel. This extra abstraction reduces performance compared to <code>data volumes</code> which write directly to the filesystem.</li> </ul> <p>Docker provides two options to store files in the host machine: <code>volumes</code> and <code>bind mounts</code>. If you're running Docker on Linux, you can also use a <code>tmpfs mount</code>, and with Docker on Windows you can also use a <code>named pipe</code>.</p> <p></p> <ul> <li><code>Volumes</code> are stored in the host filesystem that is managed by Docker.</li> <li><code>Bind mounts</code> are stored anywhere on the host system.</li> <li><code>tmpfs mounts</code> are stored in the host memory only.</li> </ul> <p>Originally, the <code>--mount</code> flag was used for Docker Swarm services and the <code>--volume</code> flag was used for standalone containers. From Docker 17.06 and higher, you can also use <code>--mount</code> for standalone containers and it is in general more explicit and verbose than <code>--volume</code>.</p>"},{"location":"lab-3/#volumes","title":"Volumes","text":"<p>A <code>data volume</code> or <code>volume</code> is a directory that bypasses the <code>Union File System</code> of Docker.</p> <p>There are three types of volumes:</p> <ul> <li>anonymous volume,</li> <li>named volume, and</li> <li>host volume.</li> </ul>"},{"location":"lab-3/#anonymous-volume","title":"Anonymous Volume","text":"<p>Let's create an instance of a popular open source NoSQL database called CouchDB and use an <code>anonymous volume</code> to store the data files for the database.</p> <p>To run an instance of CouchDB, use the CouchDB image from Docker Hub at https://hub.docker.com/_/couchdb. The docs say that the default for CouchDB is to <code>write the database files to disk on the host system using its own internal volume management</code>.</p> <p>Run the following command,</p> <pre><code>docker run -d -p 5984:5984 --name my-couchdb -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1\n</code></pre> <p>CouchDB will create an anonymous volume and generated a hashed name. Check the volumes on your host system,</p> <pre><code>$ docker volume ls\nDRIVER    VOLUME NAME\nlocal    f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\n</code></pre> <p>Set an environment variable <code>VOLUME</code> with the value of the generated name,</p> <pre><code>export VOLUME=f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\n</code></pre> <p>And inspect the volume that was created, use the hash name that was generated for the volume,</p> <pre><code>$ docker volume inspect $VOLUME\n[\n    {\n        \"CreatedAt\": \"2020-09-24T14:10:07Z\",\n        \"Driver\": \"local\",\n        \"Labels\": null,\n        \"Mountpoint\": \"/var/lib/docker/volumes/f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50/_data\",\n        \"Name\": \"f543c5319ebd96b7701dc1f2d915f21b095dfb35adbb8dc851630e098d526a50\",\n        \"Options\": null,\n        \"Scope\": \"local\"\n    }\n]\n</code></pre> <p>You see that Docker has created and manages a volume in the Docker host filesystem under <code>/var/lib/docker/volumes/$VOLUME_NAME/_data</code>. Note that this is not a path on the host machine, but a part of the Docker managed filesystem.</p> <p>Create a new database <code>mydb</code> and insert a new document with a <code>hello world</code> message.</p> <pre><code>curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb\ncurl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}'\n</code></pre> <p>Stop the container and start the container again,</p> <pre><code>docker stop my-couchdb\ndocker start my-couchdb\n</code></pre> <p>Retrieve the document in the database to test that the data was persisted,</p> <pre><code>$ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/_all_docs\n{\"total_rows\":1,\"offset\":0,\"rows\":[\n{\"id\":\"1\",\"key\":\"1\",\"value\":{\"rev\":\"1-c09289617e06b96bc747fb1201fea7f1\"}}\n]}\n\n$ curl -X GET -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1\n{\"_id\":\"1\",\"_rev\":\"1-c09289617e06b96bc747fb1201fea7f1\",\"msg\":\"hello world\"}\n</code></pre>"},{"location":"lab-3/#sharing-volumes","title":"Sharing Volumes","text":"<p>You can share an anonymous volume with another container by using the <code>--volumes-from</code> option.</p> <p>Create a <code>busybox</code> container with an anonymous volume mounted to a directory <code>/data</code> in the container, and using shell commands, write a message to a log file.</p> <pre><code>$ docker run -it --name busybox1 -v /data busybox sh\n/ # echo \"hello from busybox1\" &gt; /data/hi.log\n/ # ls /data\nhi.log\n/ # exit\n</code></pre> <p>Make sure the container <code>busybox1</code> is stopped but not removed.</p> <pre><code>$ docker ps -a\nCONTAINER ID    IMAGE    COMMAND    CREATED    STATUS    PORTS    NAMES\n437fb4a271c1    busybox    \"sh\"    18 seconds ago    Exited (0) 4 seconds ago    busybox1\n</code></pre> <p>Then create a second <code>busybox</code> container named <code>busybox2</code> using the <code>--volumes-from</code> option to share the volume created by <code>busybox1</code>,</p> <pre><code>$ docker run --rm -it --name busybox2 --volumes-from busybox1 busybox sh\n/ # ls -al /data\n/ # cat /data/hi.log\nhello from busybox1\n/ # exit\n</code></pre> <p>Docker created the anynomous volume that you were able to share using the <code>--volumes-from</code> option, and created a new anonymous volume.</p> <pre><code>$ docker volume ls\nDRIVER    VOLUME NAME\nlocal    83a3275e889506f3e8ff12cd50f7d5b501c1ace95672334597f9a071df439493\nlocal    f4e6b9f9568eeb165a56b2946847035414f5f9c2cad9ff79f18e800277ae1ebd\n</code></pre> <p>Cleanup the existing volumes and container.</p> <pre><code>docker stop my-couchdb\ndocker rm my-couchdb\ndocker rm busybox1\ndocker volume rm $(docker volume ls -q)\ndocker system prune -a\nclear\n</code></pre>"},{"location":"lab-3/#named-volume","title":"Named Volume","text":"<p>A <code>named volume</code> and <code>anonymous volume</code> are similar in that Docker manages where they are located. However, a <code>named volume</code> can be referenced by name when mounting it to a container directory. This is helpful if you want to share a volume across multiple containers.</p> <p>First, create a <code>named volume</code>,</p> <pre><code>docker volume create my-couchdb-data-volume\n</code></pre> <p>Verify the volume was created,</p> <pre><code>$ docker volume ls\nDRIVER    VOLUME NAME\nlocal    my-couchdb-data-volume\n</code></pre> <p>Now create the CouchDB container using the <code>named volume</code>,</p> <pre><code>docker run -d -p 5984:5984 --name my-couchdb -v my-couchdb-data-volume:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1\n</code></pre> <p>Wait until the CouchDB container is running and the instance is available.</p> <p>Create a new database <code>mydb</code> and insert a new document with a <code>hello world</code> message.</p> <pre><code>curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb\ncurl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}'\n</code></pre> <p>It now is easy to share the volume with another container. For instance, read the content of the volume using the <code>busybox</code> image, and share the <code>my-couchdb-data-volume</code> volume by mounting the volume to a directory in the <code>busybox</code> container.</p> <pre><code>$ docker run --rm -it --name busybox -v my-couchdb-data-volume:/myvolume busybox sh\n/ # ls -al /myvolume/\ntotal 40\ndrwxr-xr-x    4 5984    5984    4096 Sep 24 17:11 .\ndrwxr-xr-x    1 root    root    4096 Sep 24 17:14 ..\ndrwxr-xr-x    2 5984    5984    4096 Sep 24 17:11 .delete\n-rw-r--r--    1 5984    5984    8388 Sep 24 17:11 _dbs.couch\n-rw-r--r--    1 5984    5984    8385 Sep 24 17:11 _nodes.couch\ndrwxr-xr-x    4 5984    5984    4096 Sep 24 17:11 shards\n/ # exit\n</code></pre> <p>You can check the Docker managed filesystem for volumes by running a busybox container with privileged permission and set the process id to <code>host</code> to inspect the host system, and browse to the Docker managed directories.</p> <pre><code>docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh\n/ # ls -l /var/lib/docker/volumes\ntotal 28\n-rw-------    1 root     root         32768 Nov 10 15:54 metadata.db\ndrwxr-xr-x    3 root     root          4096 Nov 10 15:54 my-couchdb-data-volume\n/ # exit\n</code></pre> <p>Cleanup,</p> <pre><code>docker stop my-couchdb\ndocker rm my-couchdb\ndocker volume rm my-couchdb-data-volume\ndocker system prune -a\ndocker volume prune\nclear\n</code></pre>"},{"location":"lab-3/#host-volume","title":"Host Volume","text":"<p>When you want to access the volume directory easily from the host machine directly instead of using the Docker managed directories, you can create a <code>host volume</code>.</p> <p>Let's use a directory in the current working directory (indicated with the command <code>pwd</code>) called <code>data</code>, or choose your own data directory on the host machine, e.g. <code>/home/couchdb/data</code>. We let docker create the <code>$(pwd)/data</code> directory if it does not exist yet. We mount the <code>host volume</code> inside the CouchDB container to the container directory <code>/opt/couchdb/data</code>, which is the default data directory for CouchDB.</p> <p>Run the following command,</p> <pre><code>docker run -d -p 5984:5984 --name my-couchdb -v $(pwd)/data:/opt/couchdb/data -e COUCHDB_USER=admin -e COUCHDB_PASSWORD=passw0rd1 couchdb:3.1\n</code></pre> <p>Verify that a directory <code>data</code> was created,</p> <pre><code>$ ls -al\ntotal 16\ndrwxrwsrwx 3 root users 4096 Sep 24 16:27 .\ndrwxrwxr-x 1 root root  4096 Jul 16 20:04 ..\ndrwxr-sr-x 3 5984  5984 4096 Sep 24 16:27 data\n</code></pre> <p>and that CouchDB has created data files here,</p> <pre><code>$ ls -al data\ntotal 32\ndrwxr-sr-x 3 5984  5984 4096 Sep 24 16:27 .\ndrwxrwsrwx 3 root users 4096 Sep 24 16:27 ..\n-rw-r--r-- 1 5984  5984 4257 Sep 24 16:27 _dbs.couch\ndrwxr-sr-x 2 5984  5984 4096 Sep 24 16:27 .delete\n-rw-r--r-- 1 5984  5984 8385 Sep 24 16:27 _nodes.couch\n</code></pre> <p>Also check that now, no managed volume was created by docker, because we are now using a <code>host volume</code>.</p> <pre><code>docker volume ls\n</code></pre> <p>and</p> <pre><code>docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh\n/ # ls -l /var/lib/docker/volumes\ntotal 24\n-rw-------    1 root     root         32768 Nov 10 16:00 metadata.db\n/ # exit\n</code></pre> <p>Create a new database <code>mydb</code> and insert a new document with a <code>hello world</code> message.</p> <pre><code>curl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb\ncurl -X PUT -u admin:passw0rd1 http://127.0.0.1:5984/mydb/1 -d '{\"msg\": \"hello world\"}'\n</code></pre> <p>Note that CouchDB created a folder <code>shards</code>,</p> <pre><code>$ ls -al data\ntotal 40\ndrwxr-sr-x 4 5984  5984 4096 Sep 24 16:49 .\ndrwxrwsrwx 3 root users 4096 Sep 24 16:49 ..\n-rw-r--r-- 1 5984  5984 8388 Sep 24 16:49 _dbs.couch\ndrwxr-sr-x 2 5984  5984 4096 Sep 24 16:49 .delete\n-rw-r--r-- 1 5984  5984 8385 Sep 24 16:49 _nodes.couch\ndrwxr-sr-x 4 5984  5984 4096 Sep 24 16:49 shards\n</code></pre> <p>List the content of the <code>shards</code> directory,</p> <pre><code>$ ls -al data/shards\ntotal 16\ndrwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 .\ndrwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 ..\ndrwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 00000000-7fffffff\ndrwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 80000000-ffffffff\n</code></pre> <p>and the first shard,</p> <pre><code>$ ls -al data/shards/00000000-7fffffff/\ntotal 20\ndrwxr-sr-x 2 5984 5984 4096 Sep 24 16:49 .\ndrwxr-sr-x 4 5984 5984 4096 Sep 24 16:49 ..\n-rw-r--r-- 1 5984 5984 8346 Sep 24 16:49 mydb.1600966173.couch\n</code></pre> <p>A shard is a horizontal partition of data in a database. Partitioning data into shards and distributing copies of each shard to different nodes in a cluster gives the data greater durability against node loss. CouchDB automatically shards databases and distributes the subsets of documents among nodes.</p> <p>Cleanup,</p> <pre><code>docker stop my-couchdb\ndocker rm my-couchdb\nsudo rm -rf $(pwd)/data\ndocker system prune -a\n</code></pre>"},{"location":"lab-3/#bind-mounts","title":"Bind Mounts","text":"<p>The <code>mount</code> syntax is recommended by Docker over the <code>volume</code> syntax. Bind mounts have limited functionality compared to volumes. A file or directory is referenced by its full path on the host machine when mounted into a container. Bind mounts rely on the host machine\u2019s filesystem having a specific directory structure available and you cannot use the Docker CLI to manage bind mounts. Note that bind mounts can change the host filesystem via processes running in a container.</p> <p>Instead of using the <code>-v</code> syntax with three fields separated by colon separator (:), the <code>mount</code> syntax is more verbose and uses multiple <code>key-value</code> pairs:</p> <ul> <li>type: bind, volume or tmpfs,</li> <li>source: path to the file or directory on host machine,</li> <li>destination: path in container,</li> <li>readonly,</li> <li>bind-propagation: rprivate, private, rshared, shared, rslave, slave,</li> <li>consistency: consistent, delegated, cached,</li> <li>mount.</li> </ul> <pre><code>mkdir data\ndocker run -it --name busybox --mount type=bind,source=\"$(pwd)\"/data,target=/data busybox sh\n/ # echo \"hello busybox\" &gt; /data/hi.txt\n/ # exit\ncat data/hi.txt\n</code></pre>"},{"location":"lab-3/#optional-overlayfs","title":"[Optional] OverlayFS","text":"<p>OverlayFS is a <code>union mount filesystem</code> implementation for Linux. To understand what a Docker volume is, it helps to understand how layers and the filesystem work in Docker.</p> <p>To start a container, Docker takes the read-only image and creates a new read-write layer on top. To view the layers as one, Docker uses a Union File System or OverlayFS (Overlay File System), specifically the <code>overlay2</code> storage driver.</p> <p>To see Docker host managed files, you need access to the Docker process file system. Using the <code>--privileged</code> and <code>--pid=host</code> flags you can access the host's process ID namespace from inside a container like <code>busybox</code>. You can then browse to Docker's <code>/var/lib/docker/overlay2</code> directory to see the downloaded layers that are managed by Docker.</p> <p>To view the current list of layers in Docker,</p> <pre><code>$ docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh\n\n/ # ls -l /var/lib/docker/overlay2\ntotal 16\ndrwx------    3 root     root          4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32\ndrwx------    5 root     root          4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d\ndrwx------    4 root     root          4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init\ndrwx------    2 root     root          4096 Sep 25 19:44 l\n\n/ # exit\n</code></pre> <p>Pull down the <code>ubuntu</code> image and check again,</p> <pre><code>$ docker pull ubuntu\nUsing default tag: latest\nlatest: Pulling from library/ubuntu\ne6ca3592b144: Pull complete\n534a5505201d: Pull complete\n990916bd23bb: Pull complete\nDigest: sha256:cbcf86d7781dbb3a6aa2bcea25403f6b0b443e20b9959165cf52d2cc9608e4b9\nStatus: Downloaded newer image for ubuntu:latest\n\n$ docker run -it --privileged --pid=host busybox nsenter -t 1 -m -u -n -i sh\n\n/ # ls -l /var/lib/docker/overlay2/\ntotal 36\ndrwx------    3 root     root          4096 Sep 25 19:44 0e55ecaa4d17c353191e68022d9a17fde64fb5e9217b07b5c56eb4c74dad5b32\ndrwx------    4 root     root          4096 Sep 25 19:45 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d\ndrwx------    4 root     root          4096 Sep 25 19:44 187854d05ccd18980642e820b0d2be6a127ba85d8ed96315bb5ae37eb1add36d-init\ndrwx------    4 root     root          4096 Sep 25 19:46 a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779\ndrwx------    3 root     root          4096 Sep 25 19:46 d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65\ndrwx------    4 root     root          4096 Sep 25 19:46 dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6\ndrwx------    5 root     root          4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709\ndrwx------    4 root     root          4096 Sep 25 19:47 f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709-init\ndrwx------    2 root     root          4096 Sep 25 19:47 l\n\n/ # exit\n</code></pre> <p>You see that pulling down the <code>ubuntu</code> image, implicitly pulled down 4 new layers,</p> <ul> <li>a611792b4cac502995fa88a888261dfba0b5d852e72f9db9e075050991423779</li> <li>d181f1a41fc35a45c16e8bfcb8eee6f768f3b98f82210a43ea65f284a45fcd65</li> <li>dac2f37f6280a076836d39b87b0ae5ebf5c0d386b6d8b991b103aadbcebaa7c6</li> <li>f3e921b440c37c86d06cd9c9fb70df50edad553c36cc87f84d5eeba734aae709</li> </ul> <p>The <code>overlay2</code> storage driver in essence layers different directories on the host and presents them as a single directory.</p> <ul> <li>base layer or lowerdir,</li> <li><code>diff</code> layer or upperdir,</li> <li>overlay layer (user view), and</li> <li><code>work</code> dir.</li> </ul> <p>OverlayFS refers to the lower directories as <code>lowerdir</code>, which contains the base image and the read-only (R/O) layers that are pulled down.</p> <p>The upper directory is called <code>upperdir</code> and is the read-write (R/W) container layer.</p> <p>The unified view or <code>overlay</code> layer is called <code>merged</code>.</p> <p>Finally, a <code>workdir</code> is a required, which is an empty directory used by overlay for internal use.</p> <p>The <code>overlay2</code> driver supports up to 128 lower OverlayFS layers. The <code>l</code> directory contains shortened layer identifiers as symbolic links.</p> <p></p> <p>Cleanup,</p> <pre><code>docker system prune -a\nclear\n</code></pre>"},{"location":"lab-x/","title":"Lab 3 - Introduction to Orchestration","text":""},{"location":"lab-x/#overview","title":"Overview","text":"<p>So far you have learned how to run applications using docker on your local machine, but what about running dockerized applications in production? There are a number of problems that come with building an application for production: scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging... just to name a few.</p> <p>There are several orchestration solutions out there that help you solve some of these problems. The most widely adopted orchestration platform is Kubernetes. The IBM Kubernetes Service is a managed service of Kubernetes to run containers in production.</p> <p>Red Hat Openshift is an extension of Kubernetes with Enterprise grade services to provide a more secure and integrated extension of Kubernetes. Red Hat OpenShift Kubernetes Service (ROKS) on IBM Cloud is a managed service of OpenShift. You can also create a free 1-month, 1 node OpenShift cluster on OpenShift Online.</p> <p>This lab is for those who are interested in learning how to orchestrate applications using Docker Swarm. Docker Swarm is the orchestration tool that comes built-in to the Docker Engine.</p> <p>We will be using a few Docker commands in this lab. For full documentation on available commands check out the Docker official documentation.</p>"},{"location":"lab-x/#prerequisites","title":"Prerequisites","text":"<p>In order to complete a lab about orchestrating an application that is deployed across multiple hosts, you need... well, multiple hosts. Therefor, for this lab you will be using the multi-node support provided by Play with Docker. This is the easiest way to test out Docker Swarm, without having to deal with installing docker on multiple hosts yourself.</p> <p></p>"},{"location":"lab-x/#step-1-create-your-first-swarm","title":"Step 1: Create your first swarm","text":"<p>In this step, we will create our first swarm using play-with-docker.</p> <ol> <li> <p>Navigate to Play with Docker</p> </li> <li> <p>Click \"add new instance\" on the lefthand side three times to create three nodes</p> <p>Our first swarm cluster will have three nodes.</p> </li> <li> <p>Initialize the swarm on node 1</p> <pre><code>$ docker swarm init --advertise-addr eth0\nSwarm initialized: current node (vq7xx5j4dpe04rgwwm5ur63ce) is now a manager.\n\nTo add a worker to this swarm, run the following command:\n\n    docker swarm join \\\n    --token SWMTKN-1-50qba7hmo5exuapkmrj6jki8knfvinceo68xjmh322y7c8f0pj-87mjqjho30uue43oqbhhthjui \\\n    10.0.120.3:2377\n\nTo add a manager to this swarm, run 'docker swarm join-token manager' and follow the instructions.\n</code></pre> <p>You can think of docker swarm as a special \"mode\" that is activated by the command: <code>docker swarm init</code>. The <code>--advertise-addr</code> specifies the address in which the other nodes will use to join the swarm.</p> <p>This <code>docker swarm init</code> command generates a join token. The token makes sure that no malicious nodes join our swarm. We will need to use this token to join the other nodes to the swarm. For convenience, the output includes the full command <code>docker swarm join</code> which you can just copy/paste to the other nodes.</p> </li> <li> <p>On both node2 and node3, copy and run the <code>docker swarm join</code> command that was outputted to YOUR console by the last command.</p> <p>You now have a three node swarm!</p> </li> <li> <p>Back on node1, run <code>docker node ls</code> to verify your 3 node cluster.</p> <pre><code>$ docker node ls\nID    HOSTNAME    STATUS    AVAILABILITY    MANAGER STATUS\n7x9s8baa79l29zdsx95i1tfjp    node3    Ready    Active\nx223z25t7y7o4np3uq45d49br    node2    Ready    Active\nzdqbsoxa6x1bubg3jyjdmrnrn *   node1   Ready    Active    Leader\n</code></pre> <p>This command outputs the three nodes in our swarm. The * next to the ID of the node represents the node that handled that specific command (<code>docker node ls</code> in this case).</p> <p>Our node consists of  1 manager node and 2 workers nodes. Managers handle commands and manage the state of the swarm. Workers cannot handle commands and are simply used to run containers at scale. By default, managers are also used to run containers.</p> <p>All <code>docker service</code> commands for the rest of this lab need to be executed on the manager node (Node1).</p> <p>Note: While we will control the Swarm directly from the node in which its running, you can control a docker swarm remotely by connecting to the docker engine of the manager via the remote API or activating a remote host from your local docker installation (using the <code>$DOCKER_HOST</code> and <code>$DOCKER_CERT_PATH</code> environment variables). This will become useful when you want to control production applications remotely instead of ssh-ing directly into production servers.</p> </li> </ol>"},{"location":"lab-x/#step-2-deploy-your-first-service","title":"Step 2: Deploy your first service","text":"<p>Now that we have our 3 node Swarm cluster initialized, let's deploy some containers. To run containers on a Docker Swarm, we want to create a service. A service is an abstraction that represents multiple containers of the same image deployed across a distributed cluster.</p> <p>Let's do a simple example using Nginx. For now we will create a service with just 1 running container, but we will scale up later.</p> <ol> <li> <p>Deploy a service using Nginx</p> <pre><code>$ docker service create --detach=true --name nginx1 --publish 80:80  --mount source=/etc/hostname,target=/usr/share/nginx/html/index.html,type=bind,ro nginx:1.18\npgqdxr41dpy8qwkn6qm7vke0q\n</code></pre> <p>This above statement is declarative, and docker swarm will actively try to maintain the state declared in this command unless explicitly changed via another <code>docker service</code> command. This behavior comes in handy when nodes go down, for example, and containers are automatically rescheduled on other nodes. We will see a demonstration of that a little later on in this lab.</p> <p>The <code>--mount</code> flag is a neat trick to have nginx print out the hostname of the node it's running on. This will come in handy later in this lab when we start load balancing between multiple containers of nginx that are distributed across different nodes in the cluster, and we want to see which node in the swarm is serving the request.</p> <p>We are using nginx tag \"1.18\" in this command. We will demonstrate a rolling update with version 1.19 later in this lab.</p> <p>The <code>--publish</code> command makes use of the swarm's built in routing mesh. In this case port 80 is exposed on every node in the swarm. The routing mesh will route a request coming in on port 80 to one of the nodes running the container.</p> </li> <li> <p>Inspect the service</p> <p>You can use <code>docker service ls</code> to inspect the service you just created.</p> <pre><code>$ docker service ls\nID                  NAME                MODE                REPLICAS            IMAGE               PORTS\npgqdxr41dpy8        nginx1              replicated          1/1                 nginx:1.18          *:80-&gt;80/tcp\n</code></pre> </li> <li> <p>Check out the running container of the service</p> <p>To take a deeper look at the running tasks, you can use <code>docker service ps</code>. A task is yet another abstraction using in docker swarm that represents the running instances of a service. In this case, there is a 1-1 mapping between a task and a container.</p> <pre><code>$ docker service ps nginx1\nID    NAME    IMAGE    NODE    DESIRED STATE    CURRENT STAT\nE     ERROR    PORTS\niu3ksewv7qf9    nginx1.1    nginx:1.18    node1    Running    Running 8 minutes ago\n</code></pre> <p>If you happen to know which node your container is running on (you can see which node based on the output from <code>docker service ps</code>), you can use <code>docker container ls</code> to see the container running on that specific node.</p> </li> <li> <p>Test the service</p> <p>Because of the routing mesh, we can send a request to any node of the swarm on port 80. This request will be automatically routed to the one node that is running our nginx container.</p> <p>Try this on each node:</p> <pre><code>$ curl localhost:80\nnode1\n</code></pre> <p>Curling will output the hostname where the container is running. For this example, it is running on \"node1\", but yours might be different.</p> </li> </ol>"},{"location":"lab-x/#step-3-scale-your-service","title":"Step 3: Scale your service","text":"<p>In production we may need to handle large amounts of traffic to our application. So let's scale!</p> <ol> <li> <p>Update your service with an updated number of replicas</p> <p>We are going to use the <code>docker service</code> command to update the nginx service we created earlier to include 5 replicas. This is defining a new state for our service.</p> <pre><code>$ docker service update --replicas=5 --detach=true nginx1\nnginx1\n</code></pre> <p>As soon as this command is run the following happens:</p> <ol> <li>The state of the service is updated to 5 replicas (which is stored in the swarms internal storage).</li> <li>Docker swarm recognizes that the number of replicas that is scheduled now does not match the declared state of 5.</li> <li>Docker swarm schedules 5 more tasks (containers) in an attempt to meet the declared state for the service.</li> </ol> <p>This swarm is actively checking to see if the desired state is equal to actual state, and will attempt to reconcile if needed.</p> </li> <li> <p>Check the running instances</p> <p>After a few seconds, you should see that the swarm did its job, and successfully started 9 more containers. Notice that the containers are scheduled across all three nodes of the cluster. The default placement strategy that is used to decide where new containers are to be run is \"emptiest node\", but that can be changed based on your need.</p> <pre><code>$ docker service ps nginx1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STAT\nE            ERROR               PORTS\niu3ksewv7qf9        nginx1.1            nginx:1.18          node1               Running             Running 17 m\ninutes ago\nlfz1bhl6v77r        nginx1.2            nginx:1.18          node2               Running             Running 6 mi\nnutes ago\nqururb043dwh        nginx1.3            nginx:1.18          node3               Running             Running 6 mi\nnutes ago\nq53jgeeq7y1x        nginx1.4            nginx:1.18          node3               Running             Running 6 mi\nnutes ago\nxj271k2829uz        nginx1.5            nginx:1.18          node1               Running             Running 7 mi\nnutes ago\n</code></pre> </li> <li> <p>Send a bunch of requests to localhost:80</p> <p>The <code>--publish 80:80</code> is still in effect for this service, that was not changed when we ran <code>docker service update</code>. However, now when we send requests on port 80, the routing mesh has multiple containers in which to route requests to. The routing mesh acts as a load balancer for these containers, alternating where it routes requests to.</p> <p>Let's try it out by curling multiple times. Note, that it doesn't matter which node you send the requests. There is no connection between the node that receives the request, and the node that that request is routed to.</p> <pre><code>$ curl localhost:80\nnode3\n$ curl localhost:80\nnode3\n$ curl localhost:80\nnode2\n$ curl localhost:80\nnode1\n$ curl localhost:80\nnode1\n</code></pre> </li> </ol> <p>You should see which node is serving each request because of the nifty <code>--mount</code> command we used earlier.</p> <p>Limits of the routing Mesh The routing mesh can only publish one service on port 80. If you want multiple services exposed on port 80, then you can use an external application load balancer outside of the swarm to accomplish this.</p> <ol> <li> <p>Check the aggregated logs for the service</p> <p>Another easy way to see which nodes those requests were routed to is to check the aggregated logs. We can get aggregated logs for the service using <code>docker service logs [service name]</code>. This aggregates the output from every running container, i.e. the output from <code>docker container logs [container name]</code>.</p> <pre><code>$ docker service logs nginx1\nnginx1.4.q53jgeeq7y1x@node3    | 10.255.0.2 - - [28/Jun/2017:18:59:39 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\nnginx1.2.lfz1bhl6v77r@node2    | 10.255.0.2 - - [28/Jun/2017:18:59:40 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\nnginx1.5.xj271k2829uz@node1    | 10.255.0.2 - - [28/Jun/2017:18:59:41 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\nnginx1.1.iu3ksewv7qf9@node1    | 10.255.0.2 - - [28/Jun/2017:18:50:23 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\nnginx1.1.iu3ksewv7qf9@node1    | 10.255.0.2 - - [28/Jun/2017:18:59:41 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\nnginx1.3.qururb043dwh@node3    | 10.255.0.2 - - [28/Jun/2017:18:59:38 +0000] \"GET / HTTP/1.1\" 200 6 \"-\" \"curl/7.\n52.1\" \"-\"\n</code></pre> <p>Based on these logs we can see that each request was served by a different container.</p> <p>In addition to seeing whether the request was sent to node1, node2, or node3, you can also see which container on each node that it was sent to. For example <code>nginx1.5</code> means that request was sent to container with that same name as indicated in the output of <code>docker service ps nginx1</code>.</p> </li> </ol>"},{"location":"lab-x/#step-4-rolling-updates","title":"Step 4: Rolling Updates","text":"<p>Now that we have our service deployed, let's demonstrate a release of our application. We are going to update the version of Nginx to version \"1.19\". To do this update we are going to use the <code>docker service update</code> command.</p> <pre><code>$ docker service update --image nginx:1.19 --detach=true nginx1\n1a2b3c\n</code></pre> <p>This will trigger a rolling update of the swarm. Quickly type in <code>docker service ps nginx1</code> over and over to see the updates in real time.</p> <p>You can fine tune the rolling update using these options:</p> <ul> <li><code>--update-parallelism</code> will dictate the number of containers to update at once. (defaults to 1)</li> <li><code>--update-delay</code> will dictate the delay between finishing updating a set of containers before moving on to the next set.</li> </ul> <p>After a few seconds, run <code>docker service ps nginx1</code> to see all the images have been updated to nginx:1.19.</p> <pre><code>$ docker service ps nginx1\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE             ERROR               PORTS\nl4s05d18j9ga        nginx1.1            nginx:1.19          node1               Ready               Ready 1 second ago\nzsm6as2ffq8g         \\_ nginx1.1        nginx:1.18          node1               Shutdown            Running 1 second ago\nnk9awj3zercp        nginx1.2            nginx:1.19          node2               Running             Running 15 seconds ago\n21gxmjea135j         \\_ nginx1.2        nginx:1.18          node3               Shutdown            Shutdown 16 seconds ago\n2cdadovtek2m        nginx1.3            nginx:1.19          node1               Running             Running 6 seconds ago\nld0zgx5et2xy         \\_ nginx1.3        nginx:1.18          node1               Shutdown            Shutdown 6 seconds ago\np3gu8wcpmeax        nginx1.4            nginx:1.19          node3               Running             Running 1 second ago\nwo4ioe8lrju6         \\_ nginx1.4        nginx:1.18          node2               Shutdown            Shutdown 2 seconds ago\nlpe6dkkr4osc        nginx1.5            nginx:1.19          node3               Running             Running 10 seconds ago\n6qvqfb6x77u4         \\_ nginx1.5        nginx:1.18          node3               Shutdown            Shutdown 11 seconds ago\n</code></pre> <p>Repeat the above command, until all images have successfully updated  to the latest version of nginx!</p>"},{"location":"lab-x/#step-5-reconciliation","title":"Step 5: Reconciliation","text":"<p>In the previous step, we updated the state of our service using <code>docker service update</code>. We saw Docker Swarm in action as it recognized the mismatch between desired state and actual state, and attempted to solve this issue.</p> <p>The \"inspect-&gt;adapt\" model of docker swarm enables it to perform reconciliation when something goes wrong. For example, when a node in the swarm goes down it might take down running containers with it. The swarm will recognize this loss of containers, and will attempt to reschedule containers on available nodes in order to achieve the desired state for that service.</p> <p>We are going to remove a node, and see tasks of our nginx1 service be rescheduled on other nodes automatically.</p> <ol> <li> <p>For the sake of clean output, first create a brand new service by copying the line below. We will change the name, and the publish port to avoid conflicts with our existing service. We will also add the <code>--replicas</code> command to scale the service with 5 instances.</p> <pre><code>$ docker service create --detach=true --name nginx2 --replicas=5 --publish 81:80  --mount source=/etc/hostname,target=/usr/share/nginx/html/index.html,type=bind,ro nginx:1.18\naiqdh5n9fyacgvb2g82s412js\n</code></pre> </li> <li> <p>On Node1, use <code>watch</code> to watch the update from the output of <code>docker service ps</code>. Note \"watch\" is a linux utility and might not be available on other platforms.</p> <pre><code>$ watch -n 1 docker service ps nginx2\nok\n</code></pre> <p>This should result in a window that looks like this:</p> <pre><code>Every 1.0s: docker service ps nginx2          2020-09-23 20:59:43\n\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT\nSTATE            ERROR               PORTS\nnveflkbbzhia        nginx2.1            nginx:1.18          node1               Running             Running\n41 seconds ago\nqlk6avfcjqft        nginx2.2            nginx:1.18          node2               Running             Running\n41 seconds ago\npsizpiwxt1ta        nginx2.3            nginx:1.18          node3               Running             Running\n41 seconds ago\n0kmrqeneqztk        nginx2.4            nginx:1.18          node2               Running             Running\n41 seconds ago\nzdg9j4aiqt8w        nginx2.5            nginx:1.18          node3               Running             Running\n41 seconds ago\n</code></pre> </li> <li> <p>Click on Node3, and type the command to leave the swarm cluster.</p> <pre><code>$ docker swarm leave\nok\n</code></pre> <p>This is the \"nice\" way to leave the swarm, but you can also kill the node and the following behavior will be the same.</p> </li> <li> <p>Click on Node1 to watch the reconciliation in action. You should see that the swarm will attempt to get back to the declared state by rescheduling the containers that were running on node3 to node1 and node2 automatically.</p> <pre><code>$ docker service ps nginx2\nID                  NAME                IMAGE               NODE                DESIRED STATE       CURRENT STATE            ERROR               PORTS\njeq4604k1v9k        nginx2.1            nginx:1.18          node1               Running             Running 5 seconds ago\n6koehbhsfbi7         \\_ nginx2.1        nginx:1.18          node3               Shutdown            Running 21 seconds ago\ndou2brjfr6lt        nginx2.2            nginx:1.18          node1               Running             Running 26 seconds ago\n8jc41tgwowph        nginx2.3            nginx:1.18          node2               Running             Running 27 seconds ago\nn5n8zryzg6g6        nginx2.4            nginx:1.18          node1               Running             Running 26 seconds ago\ncnofhk1v5bd8        nginx2.5            nginx:1.18          node2               Running             Running 27 seconds ago\n[node1] (loc\n</code></pre> </li> </ol>"},{"location":"lab-x/#number-of-nodes","title":"Number of nodes","text":"<p>In this lab, our Docker Swarm cluster consists of one master, and two worker nodes. This configuration is not highly available. The manager node contains the necessary information to manage the cluster, so if this node goes down, the cluster will cease to function. For a production application, you will want to provision a cluster with multiple manager nodes to allow for manager node failures.</p> <p>For manager nodes you want at least 3, but typically no more than 7. Managers implement the raft consensus algorithm, which requires that more than 50% of the nodes agree on the state that is being stored for the cluster. If you don't achieve &gt;50%, the swarm will cease to operate correctly. For this reason, the following can be assumed about node failure tolerance.</p> <ul> <li>3 manager nodes tolerates 1 node failure</li> <li>5 manager nodes tolerates 2 node failures</li> <li>7 manager nodes tolerates 3 node failures</li> </ul> <p>It is possible to have an even number of manager nodes, but it adds no value in terms of the number of node failures. For example, 4 manager nodes would only tolerate 1 node failure, which is the same tolerance as a 3 manager node cluster. The more manager nodes you have, the harder it is to achieve a consensus on the state of a cluster.</p> <p>While you typically want to limit the number of manager nodes to no more than 7, you can scale the number of worker nodes much higher than that. Worker nodes can scale up into the 1000's of nodes. Worker nodes communicate using the gossip protocol, which is optimized to be highly performant under large traffic and a large number of nodes.</p> <p>If you are using Play with Docker, you can easily deploy multiple manager node clusters using the built in templates. Click the templates icon in the upper left to see what templates are available.</p>"},{"location":"lab-x/#summary","title":"Summary","text":"<p>In this lab, you got an introduction to problems that come with running container with production such as scheduling services across distributed nodes, maintaining high availability, implementing reconciliation, scaling, and logging. We used the orchestration tool that comes built-in to the Docker Engine- Docker Swarm, to address some of these issues.</p> <p>Key Takeaways:</p> <ul> <li>The Docker Swarm schedules services using a declarative language. You declare the state, and the swarm attempts to maintain and reconcile to make sure the actual state == desired state</li> <li>Docker Swarm is composed of manager and worker nodes. Only managers can maintain the state of the swarm and accept commands to modify it. Workers have high scability and are only  used to run containers. By default managers can run containers as well.</li> <li>The routing mesh built into swarm means that any port that is published at the service level will be exposed on every node in the swarm. Requests to a published service port will be routed automatically to a container of the service that is running in the swarm.</li> <li>Many tools out there exist to help solve problems with orchestration containerized applications in production, include Docker Swarm, and the IBM Kubernetes Service.</li> </ul>"}]}